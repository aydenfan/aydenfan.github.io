<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Welcome</title>
    <url>/2021/06/23/My-First-Blog/</url>
    <content><![CDATA[<p><img src="https://img-blog.csdnimg.cn/cover5/237143518359060527.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,image_MjAyMDA3MTUxNjIxMDEzOC5wbmc=,size_16,color_FFFFFF,t_70,image/resize,m_lfit,w_962#pic_center" alt="在这里插入图片描述"></p>
]]></content>
  </entry>
  <entry>
    <title>Self-supervised Structure Modeling for Object Recognition</title>
    <url>/2021/06/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Look-into-Object_%20Self-supervised%20Structure%20Modeling%20for%20Object%20Recognition/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Most object recognition approaches predominantly focus on learning discriminative visual patterns while overlooking the holistic object structure. Though important, structure modeling usually requires significant manual annotations and therefore is labor-intensive. In this paper, we propose to “look into object” (explicitly yet intrinsically model the object structure) through incorporating self-supervisions into the traditional framework. We show the recognition backbone can be substantially enhanced for more robust representation learning, without any cost of extra annotation and inference speed. Specifically, we first propose an object-extent learning module for localizing the object according to the visual patterns shared among the instances in the same category. We then design a spatial context learning module for modeling the internal structures of the object, through predicting the relative positions within the extent. These two modules can be easily plugged into any backbone networks during training and detached at inference time. Extensive experiments show that our look-into-object approach (LIO) achieves large performance gain on a number of benchmarks, including generic object recognition (ImageNet) and fine-grained object recognition tasks (CUB, Cars, Aircraft). We also show that this learning paradigm is highly generalizable to other tasks such as object detection and segmentation (MS COCO). Project page: <a href="https://github.com/JDAI-CV/LIO">https://github.com/JDAI-CV/LIO</a>.<span id="more"></span></p>
<p>大多数的物体识别方法主要集中在学习有区别的视觉模式，而忽略了整体的物体结构。虽然很重要，但结构建模通常需要大量的手工注释，因此是劳动密集型的。在本文中，我们建议通过将自我监督整合到传统框架中来“研究对象”(明确而本质地建模对象结构)。我们证明了识别主干可以被大大增强，以获得更健壮的表示学习，而无需额外的注释和推理速度的代价。具体来说，我们首先提出一个对象范围学习模块，用于根据同一类别中的实例之间共享的可视化模式定位对象。然后，我们设计了一个空间情境学习模块，通过预测物体在范围内的相对位置来建模物体的内部结构。这两个模块可以很容易地在训练期间插入任何骨干网，并在推理时分离。大量的实验表明，我们的寻找对象方法(LIO)在一些基准测试中获得了很大的性能提升，包括通用对象识别(ImageNet)和细粒度对象识别任务(CUB, Cars, Aircraft)。我们还表明，这种学习模式是高度可推广到其他任务，如目标检测和分割(MS COCO)。项目页面:<a href="https://github.com/JDAI-CV/LIO。">https://github.com/JDAI-CV/LIO。</a></p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><h2 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h2><p>•分类模块(CM):提取基本图像表示并产生最终对象类别的主干分类网络。<br>•对象范围学习模块(Object-Extent Learning, OEL):一个用于定位给定图像中的主要对象的模块。<br>•空间上下文学习模块(Spatial Context Learning Module, SCL):一个自我监督的模块，通过CM中特征单元(后面介绍)之间的相互作用来加强区域之间的联系。<br><img src="https://img-blog.csdnimg.cn/20210617104008566.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="分类模块-Classification-Module-CM"><a href="#分类模块-Classification-Module-CM" class="headerlink" title="分类模块 Classification Module(CM)"></a>分类模块 Classification Module(CM)</h2><p>给定图像$I$，标签$l$，通过基本网络得到特征图$f(I)$，特征图大小为$N×N×C$，通过池化全连接层得到最后的特征向量$y(I)$，通过交叉熵损失来优化网络。</p>
<script type="math/tex; mode=display">\mathcal{L}_{c l s}=-\sum_{I \in \mathcal{I}} l \cdot \log \boldsymbol{y}(I)</script><p>OEL模块和SCL模块旨在帮助我们的主干分类网络学习有利于结构理解和对象定位的表示。这两个模块都是轻量级的，只引入了少量的可学习参数。此外，在推理时禁用OEL和SCL，只需要分类模块来提高计算效率。</p>
<h2 id="对象范围学习模块Object-Extent-Learning-OEL"><a href="#对象范围学习模块Object-Extent-Learning-OEL" class="headerlink" title="对象范围学习模块Object-Extent Learning (OEL)"></a>对象范围学习模块Object-Extent Learning (OEL)</h2><p>1.将特征图$f(I)$在像素维度上分成$N×N$个特征向量$\boldsymbol{f}(I)_{i, j} \in \mathbb{R}^{1 \times C}$$(1 \leq i, j \leq N)$，每个特征向量集中响应输入图像I中的某个区域。<br>2.采样和图片$I$具有相同标签的数据$\boldsymbol{I}^{\prime}=\left\{I_{1}^{\prime}, I_{2}^{\prime}, \cdots, I_{P}^{\prime}\right\}$，测量$f(I)_{i, j}$与同标签其他每幅图像$I^{\prime} \in \boldsymbol{I}^{\prime}$之间的区域级相关性。</p>
<script type="math/tex; mode=display">\varphi_{i, j}\left(I, I^{\prime}\right)=\frac{1}{C} \max _{1 \leq i^{\prime}, j^{\prime} \leq N}\left\langle\boldsymbol{f}(I)_{i, j}, \boldsymbol{f}\left(I^{\prime}\right)_{i^{\prime}, j^{\prime}}\right\rangle</script><p>$\langle\cdot, \cdot\rangle$为点乘<br>—————————————————————————————————————<br><strong>关于这个公式联系下图开会时我也思考了一下：同类图片得到特征图之后，分成N*N份特征向量单元，点乘可以评估两个特征向量的相似程度，因此对每个特征单元分别进行点乘后选取最大的即表明对应的区域相似度最高，由此产生了对应于整个物体的特征。</strong><br>—————————————————————————————————————<br>3.与分类$L_{cls}$联合训练，相关分数的$\varphi_{i, j}\left(I, I^{\prime}\right)$通常与$l$的语义相关度呈正相关关系。<br>4.这种语义相关掩码$\varphi$可以很好地捕获来自同一类别的图像的共性，其中$\varphi$中的值自然地区分了主要目标区域和背景。考虑到视点变化和变形的影响，使用多个正图像来定位一个对象的主要区域。</p>
<script type="math/tex; mode=display">M\left(I, \boldsymbol{I}^{\prime}\right)=\frac{1}{P} \sum_{p=1}^{P} \varphi\left(I, I_{p}^{\prime}\right)</script><p>$M\left(I, \boldsymbol{I}^{\prime}\right)$也可以被认为是同一类别图像之间共有的表征。</p>
<p><img src="https://img-blog.csdnimg.cn/20210616095026176.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">5.在得到特征图$f(I)$后，对$f(I)$中所有特征映射进行加权融合。对特征进行1 × 1卷积处理，得到单通道的输出$m^{\prime}(I)$(实际上就是将特征集合起来以此得到目标的整体结构)。与传统的注意力的目的是检测某些特定的部分或区域不同，我们的OEL模块被训练为收集物体内部的所有区域，忽略背景或其他无关的物体。<br>OEL模块$\mathcal{L}_{\text {oel }}$可以定义为对象范围的伪掩模$M\left(I, \boldsymbol{I}^{\prime}\right)$与$m^{\prime}(I)$之间的距离:</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text {oel }}=\sum_{I \in \mathcal{I}} \operatorname{MSE}\left(m^{\prime}(I), M\left(I, \boldsymbol{I}^{\prime}\right)\right)</script><h2 id="空间上下文学习模块Spatial-Context-Learning-SCL"><a href="#空间上下文学习模块Spatial-Context-Learning-SCL" class="headerlink" title="空间上下文学习模块Spatial Context Learning (SCL)"></a>空间上下文学习模块Spatial Context Learning (SCL)</h2><p>此模块同样作用于$f(I)$，目的是学习区域之间的结构关系。首先，对$f(I)$进行1 × 1卷积加ReLU处理，得到新映射$\boldsymbol{h}(I) \in \mathbb{R}^{N \times N \times C_{1}}$，描述了不同特征元的空间信息。$\boldsymbol{h}(I)$中的每个cell(每个i, j位置)集中代表了图像$I$中一个区域的语义信息。通过建立不同区域之间的<strong>空间连接</strong>，可以很容易地建模对象不同部分之间的结构关系。<br>1.选用极坐标系来建模这种结构关系。$N×N$平面作为极坐标平面，原点为特征中值最大的位置$R_{o}=R_{x, y}$，区域$R_{i, j}$的极坐标可以写成$\left(\Gamma_{i, j}, \theta_{i, j}\right)$，此作为自监督的标签。<script type="math/tex">\begin{aligned} \Gamma_{i, j} &=\sqrt{(x-i)^{2}+(y-j)^{2}} / \sqrt{2} N \\ \theta_{i, j} &=(\operatorname{atan2}(y-j, x-i)+\pi) / 2 \pi \end{aligned}</script><br><img src="https://img-blog.csdnimg.cn/20210616201937356.png" alt="atan2"><br><img src="https://img-blog.csdnimg.cn/20210617104419668.png" alt="在这里插入图片描述"></p>
<p>2.对每个cell $h(I)_{i, j}$和原点处$h(I)_{x, y}$在通道维度上进行级联，通过全连接层和relu激活函数，得到预测的极坐标$\left(\Gamma_{i, j}^{\prime}, \theta_{i, j}^{\prime}\right)$，从OEL模块学到的对象范围掩码$m^{\prime}(I)$(one channel)也适用于SCL模块。通过以下损失函数进行优化。</p>
<script type="math/tex; mode=display">\mathcal{L}_{d i s}=\sum_{I \in \mathcal{I}} \sqrt{\frac{\sum_{1 \leq i, j \leq N} m^{\prime}(I)_{i, j}\left(\Gamma_{i, j}^{\prime}-\Gamma_{i, j}\right)^{2}}{\sum m^{\prime}(I)}}</script><script type="math/tex; mode=display">\mathcal{L}_{\angle}=\sum_{I \in \mathcal{I}} \sqrt{\frac{\sum_{1 \leq i, j \leq N} m^{\prime}(I)_{i, j}\left(\theta_{\Delta_{i, j}}-\bar{\theta}_{\Delta}\right)^{2}}{\sum m^{\prime}(I)}}</script><p>$\theta_{\Delta_{i, j}}=\left\{\begin{array}{ll}\theta_{i, j}^{\prime}-\theta_{i, j}, &amp; \text { if } \theta_{i, j}^{\prime}-\theta_{i, j} \geq 0 \\ 1+\theta_{i, j}^{\prime}-\theta_{i, j}, &amp; \text { otherwise }\end{array}\right.$<br>$\bar{\theta}_{\Delta}=\frac{1}{\sum m^{\prime}(I)} \sum_{1 \leq i, j \leq N} m^{\prime}(I)_{i, j} \theta_{\Delta_{i, j}}$<br>3.SCL模块的损失为：</p>
<script type="math/tex; mode=display">\mathcal{L}_{s c l}=\mathcal{L}_{d i s}+\mathcal{L}_{L}</script><h2 id="联合训练"><a href="#联合训练" class="headerlink" title="联合训练"></a>联合训练</h2><p>分类、对象范围学习和空间上下文学习模块以端到端方式进行训练</p>
<script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{c l s}+\alpha \mathcal{L}_{o e l}+\beta \mathcal{L}_{s c l}</script><p>$\alpha=\beta=0.1$</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>细粒度</tag>
        <tag>分类</tag>
        <tag>自监督</tag>
      </tags>
  </entry>
</search>
