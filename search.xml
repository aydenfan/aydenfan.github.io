<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Welcome</title>
    <url>/Welcome/</url>
    <content><![CDATA[<p><img src="https://img-blog.csdnimg.cn/cover5/237143518359060527.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,image_MjAyMDA3MTUxNjIxMDEzOC5wbmc=,size_16,color_FFFFFF,t_70,image/resize,m_lfit,w_962#pic_center" alt="Welcome"></p>
]]></content>
  </entry>
  <entry>
    <title>Cross-X Learning for Fine-Grained Visual Categorization</title>
    <url>/Cross-X%20Learning%20for%20Fine-Grained%20Visual%20Categorization/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Recognizing objects from subcategories with very subtle differences remains a challenging task due to the large intra-class and small inter-class variation. Recent work tackles this problem in a weakly-supervised manner: object parts are first detected and the corresponding part-specific features are extracted for fine-grained classification. However, these methods typically treat the part-specific features of each image in isolation while neglecting their relationships between different images. In this paper, we propose Cross-X learning, a simple yet effective approach that exploits the relationships between different images and between different network layers for robust multi-scale feature learning. Our approach involves two novel components: (i) a cross-category cross-semantic regularizer that guides the extracted features to represent semantic parts and, (ii) a cross-layer regularizer that improves the robustness of multi-scale features by matching the prediction distribution across multiple layers. Our approach can be easily trained end-to-end and is scalable to large datasets like NABirds. We empirically analyze the contributions of different components of our approach and demonstrate its robustness, effectiveness and state-of-the-art performance on five benchmark datasets. Code is available at https: //github.com/cswluo/CrossX.<span id="more"></span></p>
<p>从具有非常细微差异的子类别中识别对象仍然是一项具有挑战性的任务，因为类内差异很大，类间差异很小。最近的工作以一种弱监督的方式解决了这个问题：首先检测目标部分，然后提取相应的特定于部分的特征进行细粒度分类。然而，这些方法通常孤立地对待每个图像的特定部分特征，而忽略它们在不同图像之间的关系。在本文中，我们提出了Cross-X学习，这是一种简单而有效的方法，它利用不同图像之间和不同网络层之间的关系来进行稳健的多尺度特征学习。我们的方法包括两个新的组成部分：(I)跨类别跨语义正则化，用于引导提取的特征表示语义部分；(Ii)跨层正则化，通过匹配多层预测分布来提高多尺度特征的鲁棒性。我们的方法可以很容易地端到端训练，并且可以扩展到像NABirds这样的大型数据集。我们实证分析了该方法的不同组成部分的贡献，并在五个基准数据集上展示了它的健壮性、有效性和最先进的性能。代码<a href="https://github.com/cswluo/CrossX">https://github.com/cswluo/CrossX</a></p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p><img src="https://img-blog.csdnimg.cn/20210406103106940.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt=""><br>图1.网络架构。我们的网络通过使用osme块输出多个特征地图。在最后两个阶段中，描述了两个OSME块，每个块都有两个激发，以说明我们的方法。来自阶段L−1(蓝色)和L(红色)的特征地图被组合以生成合并的特征图(橙色)。左上角是合并特征图的合并过程的放大显示。然后，通过GAP或GMP聚合特征图以获得相应的集合特征。来自同一阶段的融合特征被C3S正则化相互约束，并且同时被连接以馈送到fc层生成逻辑值。在转换为类概率后，通过CL正则化对逻辑进行约束，并将其组合用于分类。<br>———————————————————————————————————————</p>
<h2 id="OSME模块"><a href="#OSME模块" class="headerlink" title="OSME模块"></a>OSME模块</h2><p>OSME（one-squeeze multi-excitation）模块结合代码和结构图给更容易理解：<br><img src="https://img-blog.csdnimg.cn/20210406162723433.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt=""><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">class MELayer(nn.Module):</span><br><span class="line">    def __init__(self, channel, reduction=16, nparts=1):</span><br><span class="line">        super(MELayer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(1)</span><br><span class="line">        self.nparts = nparts</span><br><span class="line">        parts = list()</span><br><span class="line">        <span class="keyword">for</span> part <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">            parts.append(nn.Sequential(</span><br><span class="line">                nn.Linear(channel, channel // reduction),</span><br><span class="line">                nn.ReLU(inplace=True),</span><br><span class="line">                nn.Linear(channel // reduction, channel),</span><br><span class="line">                nn.Sigmoid()</span><br><span class="line">            ))</span><br><span class="line">        self.parts = nn.Sequential(*parts)</span><br><span class="line">       </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c)</span><br><span class="line"></span><br><span class="line">        meouts = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">            meouts.append(x * self.parts[i](y).view(b, c, 1, 1))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> meouts</span><br></pre></td></tr></table></figure></p>
<p>设$U=\left[u_{1}, \cdots, u_{C}\right] \in \mathbb{R}^{W \times H \times C}$表示残差块$\tau$的输出特征图。为了生成多个特定于注意的特征图，OSME块通过执行单次压缩(池化)和多次激发(FC)操作来扩展原始残差块。虽然OSME可以产生特定注意力的特征，但引导这些特征具有语义意义是具有挑战性的。</p>
<h2 id="跨类别跨语义正则化-C-3-S"><a href="#跨类别跨语义正则化-C-3-S" class="headerlink" title="跨类别跨语义正则化($C^{3} S$)"></a>跨类别跨语义正则化($C^{3} S$)</h2><p>接上(挑战)，作者提出通过研究不同图像和不同激励模块的特征映射之间的相关性来学习语义特征。理想情况下，希望从同一个激发模块中提取的特征具有相同的语义含义，即使它们来自不同的带有不同类标记的图像。从不同激励模块提取的特征，即使来自同一幅图像，也应该具有不同的语义。为了实现这一目标，我们引入了跨类别跨语义正则化器（$C^{3}S$），该正则化器最大化了来自同一激励模块的特征的相关性，同时最小化了来自不同激励模块的特征的相关性。<br><img src="https://img-blog.csdnimg.cn/2021040616371292.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt=""><br>图2. $C^{3}S$示例。以中间图像为例，$C^{3}S$通过利用来自不同图像（橙色虚线框）的特征和来自不同激励模块（蓝色阴影框）的特征之间的关系，鼓励在不同语义部分激活激励模块$U1$和$U2$。<br>———————————————————————————————————————<br>正则化损失函数公式：</p>
<script type="math/tex; mode=display">\mathcal{L}_{C^{3} S}(S)=\frac{1}{2}\left(\|S\|_{F}^{2}-2\|\operatorname{diag}(S)\|_{2}^{2}\right)</script><p>$S_{p, p^{\prime}}=\frac{1}{N^{2}} \sum \mathbf{F}_{p}^{T} \mathbf{F}_{p^{\prime}}$<br>$\mathbf{f}_{p} \leftarrow \mathbf{f}_{p} /\left|\mathbf{f}_{p}\right|$<br>$\mathbf{F}_{p}=\left[\mathbf{f}_{p, 1}, \cdots, \mathbf{f}_{p, N}\right] \in \mathbb{R}^{C \times N}$<br>正则化器即正则化损失，输入是结构图中的同颜色的正方体（特征图），从以下两个部分构造正则化损失：1）最大化$S$的对角线以最大化同一激励模块内的相关性；2）惩罚$S$的范式以最小化不同激励模块之间的相关性；<br>代码实现：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">reg_loss_ulti = RegularLoss(gamma=gamma1, nparts=nparts)</span><br><span class="line">reg_loss_plty = RegularLoss(gamma=gamma2, nparts=nparts)</span><br><span class="line">reg_loss_cmbn = RegularLoss(gamma=gamma3, nparts=nparts)</span><br><span class="line"><span class="comment">###############################################</span></span><br><span class="line">outputs_ulti, outputs_plty, outputs_cmbn, ulti_ftrs, plty_ftrs, cmbn_ftrs = model(inputs)</span><br><span class="line">reg_loss_cmbn = reg_loss_cmbn(cmbn_ftrs)</span><br><span class="line">reg_loss_ulti = reg_loss_ulti(ulti_ftrs)</span><br><span class="line">reg_loss_plty = reg_loss_plty(plty_ftrs)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">class RegularLoss(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, gamma=0, part_features=None, nparts=2):</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        :param bs: batch size</span></span><br><span class="line"><span class="string">        :param ncrops: number of crops used at constructing dataset</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        super(RegularLoss, self).__init__()</span><br><span class="line">        <span class="comment">#self.register_buffer(&#x27;part_features&#x27;, part_features)</span></span><br><span class="line">        self.nparts = nparts</span><br><span class="line">        self.gamma = gamma</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        assert isinstance(x, list), <span class="string">&quot;parts features should be presented in a list&quot;</span></span><br><span class="line">        corr_matrix = torch.zeros(self.nparts, self.nparts)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">            x[i] = x[i].squeeze()</span><br><span class="line">            x[i] = torch.div(x[i], x[i].norm(dim=1, keepdim=True))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># original design</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">                corr_matrix[i, j] = torch.mean(torch.mm(x[i], x[j].t()))</span><br><span class="line">                <span class="keyword">if</span> i == j:</span><br><span class="line">                    corr_matrix[i, j] = 1.0 - corr_matrix[i, j]</span><br><span class="line">        regloss = torch.mul(torch.sum(torch.triu(corr_matrix)), self.gamma).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> regloss</span><br></pre></td></tr></table></figure>
<h2 id="跨层正则化-CL"><a href="#跨层正则化-CL" class="headerlink" title="跨层正则化($CL$)"></a>跨层正则化($CL$)</h2><p>利用CNN不同层次的语义特征对许多视觉任务都是有益的。将这种思想推广到细粒度识别的一个简单方法是将不同层的预测结果结合起来进行最终的预测。然而，这种简单的策略通常会导致较差的性能。 我们假设这个问题是由于两个原因造成的：1）中间层特征对输入变化更敏感，这使得它们对于类内变化较大的细粒度识别的鲁棒性降低；2）特征预测之间的关系没有被利用。为了解决这些问题，我们采用特征金字塔网络（FPN）来集成不同层的特征，并提出了一种新的跨层正则化方法（$CL$），通过匹配不同层之间的预测分布来学习鲁棒性特征。<br><img src="https://img-blog.csdnimg.cn/20210406214706240.png" alt=""></p>
<script type="math/tex; mode=display">\mathbf{U}_{p}^{G}=\mathbf{B} \mathbf{N}\left(\mathbf{K}_{2} *\left(\mathbf{U}_{p}^{L-1}+\operatorname{Bilinear}\left(\mathbf{K}_{1} * \mathbf{U}_{p}^{L}\right)\right)\right)</script><p>$K1$, $K2$ are 1 × 1 and 3 × 3 filters<br>Bilinear(·) denotes bilinear interpolation<br>$\mathbf{U}^{G}$综合了中间层空间分辨率高和顶层语义丰富的特点。为了进一步挖掘特征预测之间的关系，作者提出了匹配不同层之间预测分布的CL正则化器。</p>
<script type="math/tex; mode=display">\begin{aligned} \mathcal{L}_{C L}\left(\mathbf{P r}^{L}, \mathbf{P r}^{L-1}\right) &=\mathrm{KL}\left(\mathbf{P r}^{L} \| \mathbf{P r}^{L-1}\right) \\ &=\frac{1}{N} \sum_{n=1}^{N} \sum_{k=1}^{K} p_{n k}^{L} \log \frac{p_{n k}^{L}}{p_{n k}^{L-1}} \end{aligned}</script><p>$\mathbf{P r}^{L-1}=\sigma\left(f\left(U^{L-1}\right)\right)$<br>$\mathbf{P r}^{L}=\sigma\left(f\left(U^{L}\right)\right)$<br>$\sigma\left(·\right)$ is the softmax function<br>$f\left(·\right)$ denotes the output layer<br>最终的预测分类可以结合不同模块的输出：</p>
<script type="math/tex; mode=display">\mathbf{P r}=\sigma\left(f\left(\mathbf{U}^{L}\right)+f\left(\mathbf{U}^{L-1}\right)+f\left(\mathbf{U}^{G}\right)\right)</script><p>最终损失为：</p>
<script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{\text {data }}+\gamma \mathcal{L}_{C^{3} S}+\lambda \mathcal{L}_{C L}</script><script type="math/tex; mode=display">\mathcal{L}_{\text {data }}=-\frac{1}{N} \sum_{n=1}^{N} \sum_{k=1}^{K} c_{n k} \log p_{n k}</script><script type="math/tex; mode=display">\mathcal{L}_{C^{3} S}=\gamma_{1} \mathcal{L}_{C^{3} S}\left(S^{L}\right)+\gamma_{2} \mathcal{L}_{C^{3} S}\left(S^{L-1}\right)+\gamma_{3} \mathcal{L}_{C^{3} S}\left(S^{G}\right)</script><script type="math/tex; mode=display">\mathcal{L}_{C L}=\lambda_{1} \mathcal{L}_{C L}\left(\mathbf{P r}^{L}, \mathbf{P r}^{L-1}\right)+\lambda_{2} \mathcal{L}_{C L}\left(\mathbf{P r}^{L}, \mathbf{P r}^{G}\right)</script>]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>细粒度</tag>
        <tag>分类</tag>
        <tag>注意力</tag>
      </tags>
  </entry>
  <entry>
    <title>BBN Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</title>
    <url>/BBN%20Bilateral-Branch%20Network%20with%20Cumulative%20Learning%20for%20Long-Tailed%20Visual%20Recognition/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Our work focuses on tackling the challenging but natural visual recognition task of long-tailed data distribution (i.e., a few classes occupy most of the data, while most classes have rarely few samples). In the literature, class re-balancing strategies (e.g., re-weighting and re-sampling) are the prominent and effective methods proposed to alleviate the extreme imbalance for dealing with long-tailed problems. In this paper , we firstly discover that these rebalancing methods achieving satisfactory recognition accuracy owe to that they could significantly promote the classifier learning of deep networks. However, at the same time, they will unexpectedly damage the representative ability of the learned deep features to some extent. Therefore, we propose a unified Bilateral-Branch Network (BBN) to take care of both representation learning and classifier learning simultaneously, where each branch does perform its own duty separately. In particular , our BBN model is further equipped with a novel cumulative learning strategy, which is designed to first learn the universal patterns and then pay attention to the tail data gradually. Extensive experiments on four benchmark datasets, including the large-scale iNaturalist ones, justify that the proposed BBN can significantly outperform state-of-the-art methods. Furthermore, validation experiments can demonstrate both our preliminary discovery and effectiveness of tailored designs in BBN for long-tailed problems. Our method won the first place in the iNaturalist 2019 large scale species classification competition, and our code is open-source and available at <a href="https://github.com/Megvii-Nanjing/BBN">https://github.com/Megvii-Nanjing/BBN</a>.<span id="more"></span></p>
<p>我们的工作重点是解决长尾数据分布的具有挑战性但自然的视觉识别任务（即少数类占据了大部分数据，而大多数类很少有样本）。在文献中，类重平衡策略（如重加权和重采样）是解决长尾问题的突出而有效的方法。在本文中，我们首先发现，这些再平衡方法之所以能达到令人满意的识别精度，是因为它们能显著地促进深层网络的分类器学习。但同时，也会在一定程度上意外地损害所学深层特征的表征能力。因此，我们提出了一个统一的双边分支网络（BBN），同时兼顾表示学习和分类器学习，每个分支单独执行各自的任务。特别地，我们的BBN模型进一步配备了一种新的累积学习策略，即先学习通用模式，然后逐渐关注尾部数据。在四个基准数据集（包括大规模的不自然数据集）上进行的大量实验表明，所提出的BBN方法的性能明显优于现有的方法。此外，验证实验可以证明我们在BBN中针对长尾问题的定制设计的初步发现和有效性。我们的方法在2019年大型物种分类比赛中获得了第一名，我们的代码是开源的，<a href="https://github.com/Megvii-Nanjing/BBN。">https://github.com/Megvii-Nanjing/BBN。</a></p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>尽管re-weighting方法具有很好的预测效果，但这些方法仍然存在不利影响，即它们也会在一定程度上会损害所学习的深层特征（即表征学习）的表征能力。具体来说，当数据极度不平衡时，重采样有过拟合尾部数据（通过过采样）的风险，也有无法拟合整个数据分布（通过欠采样）的风险。对于重加权，它会通过直接改变甚至反转数据呈现频率来扭曲原始分布。<br><img src="https://img-blog.csdnimg.cn/20210413093139992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="Top-1 error rates of different manners for representation learning and classifier learning on two long-tailed datasets CIFAR-100-
IR50 and CIFAR-10-IR50"><br>上图表明了平衡策略对特征学习和分类学习的影响。作者采用了简单训练（传统的交叉熵）、重加权和重采样三种学习方式来获得它们相应的学习表示。然后，在分类器学习的后一阶段，首先确定前一阶段收敛的表示学习参数（即冻结主干层），然后对这些网络的分类器（即全连通层）采用前面所述的三种方式进行从头训练。<br>最终证明：<strong>平衡策略会损害表征学习，促进分类学习</strong></p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p><img src="https://img-blog.csdnimg.cn/20210413094008781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="Framework of our Bilateral-Branch Network (BBN)"><br>——————————————————————————————————————————————————<br><em>为了彻底提高长尾问题的识别性能，本文提出了一种同时兼顾表示学习和分类器学习的统一双边分支网络（BBN）模型。如上图所示，我们的BBN模型由两个分支组成，称为“传统学习分支”和“重新平衡分支”。BBN的每个分支分别对表示学习和分类器学习履行各自的职责。顾名思义，配备了典型的统一采样器w.r.t.的传统的学习分支学习原始数据分布来对通用模式进行识别。同时，设计了再平衡支路和反向采样器对尾数据进行建模。然后，通过自适应折衷参数α将这些双边分支的预测输出聚合到累积学习部分。α是由“适配器”根据训练epochs的个数自动生成的，它调整整个BBN模型，首先从原始分布中学习全局特征，然后逐步关注尾部数据。更重要的是，α可以进一步控制每个分支的参数更新，例如，在训练后期强调尾部数据时，避免了对所学习的通用特征的破坏。</em></p>
<h2 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h2><p>具体来说：两个分支分别用于表示学习和分类器学习，分别称为“传统学习分支(Conventional Learning Branch)”和“再平衡分支”(Re-balancing Branch)。两个分支使用相同的残差网络结构并且共享除最后一个残差块之外的所有权重。设$x .$表示训练样本，$y . \in\{1,2, \ldots, C\}$是它对应的标签，其中$C$是类的数目。对于双边分支，我们分别对每个分支应用均匀和反向采样，并获得两个样本$\left(\mathbf{x}_{c}, y_{c}\right)$和$\left(\mathbf{x}_{r}, y_{r}\right)$作为输入数据，其中$\left(\mathbf{x}_{c}, y_{c}\right)$表示常规学习分支，$\left(\mathbf{x}_{r}, y_{r}\right)$表示重新平衡分支。然后，将两个样本送入各自对应的分支，通过全局平均池化得到特征向量$fc$和$fr$，将两个向量送进累计学习分支，通过随训练迭代次数自调节的$\alpha$来合成新的特征向量，对此向量进行分类学习，合成公式如下：</p>
<script type="math/tex; mode=display">\mathbf{z}=\alpha \boldsymbol{W}_{c}^{\top} \boldsymbol{f}_{c}+(1-\alpha) \boldsymbol{W}_{r}^{\top} \boldsymbol{f}_{r}</script><h2 id="采样策略"><a href="#采样策略" class="headerlink" title="采样策略"></a>采样策略</h2><p>Conventional Learning Branch在一个训练epoch中，训练数据集中的每个样本以相同的概率采样一次。均匀采样器保留了原始分布的特征，有利于表征学习。然而，Re-balancing Branch的目的在于缓解极端的不平衡，特别是提高尾类（tail类）的分类精度，尾类的输入数据来自于反向采样器。对于反向取样器，每个类的采样可能性与其样本大小的倒数成正比，即类中的样本越多，类具有的采样可能性越小。</p>
<h2 id="共享权重"><a href="#共享权重" class="headerlink" title="共享权重"></a>共享权重</h2><p>BBN中，两个分支共享相同的残差网络结构。网络除了最后一个残差块，共享相同的权重。共享权重有两个好处：</p>
<ol>
<li>Conventional Learning Branch的良好学习表示有利于重平衡分支的学习；</li>
<li>在推理阶段，共享权重将大大降低计算复杂度。</li>
</ol>
<h2 id="累计学习"><a href="#累计学习" class="headerlink" title="累计学习"></a>累计学习</h2><p>通过控制两个分支产生的特征的权重和分类损失$L$，$\mathcal{L}=\alpha E\left(\hat{\boldsymbol{p}}, y_{c}\right)+(1-\alpha) E\left(\hat{\boldsymbol{p}}, y_{r}\right)$，在两个分支之间转移学习重心。首先学习通用模式，然后逐步关注尾部数据。$\mathbf{z}=\alpha \boldsymbol{W}_{c}^{\top} \boldsymbol{f}_{c}+(1-\alpha) \boldsymbol{W}_{r}^{\top} \boldsymbol{f}_{r}$。$\alpha$的计算公式为:</p>
<script type="math/tex; mode=display">\alpha=1-\left(\frac{T}{T_{\max }}\right)^{2}</script><h2 id="推理阶段"><a href="#推理阶段" class="headerlink" title="推理阶段"></a>推理阶段</h2><p>将$\alpha$设为0.5，因为两个分支同等重要</p>
<h1 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h1><p>Re-balancing Branch中使用不同的采样器，反向采样器效果最好<br><img src="https://img-blog.csdnimg.cn/20210413101330302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt=""><br>累计学习中$\alpha$的不同降低策略，抛物线最好<br><img src="https://img-blog.csdnimg.cn/20210413101443375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="代码实现（自己参照源码写的粗糙的网络）"><a href="#代码实现（自己参照源码写的粗糙的网络）" class="headerlink" title="代码实现（自己参照源码写的粗糙的网络）"></a>代码实现（自己参照源码写的粗糙的网络）</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">class Combiner(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Combiner, self).__init__()</span><br><span class="line">        self.epoch_number = 80</span><br><span class="line">        self.func = torch.nn.Softmax(dim=1)</span><br><span class="line">        self.initilize_all_parameters()</span><br><span class="line">        self.resnet = resnet50(pretrained=True)</span><br><span class="line">        self.backbone = torch.nn.Sequential(*list(self.resnet.children())[:-2])</span><br><span class="line">        self.module = nn.AdaptiveAvgPool2d((1, 1))</span><br><span class="line">        self.gender = DenseLayer(1, 32)</span><br><span class="line">        self.classifier = nn.Linear(2*(2048+32), 240)</span><br><span class="line">        self.softmax = nn.Softmax(dim=1)</span><br><span class="line">        self.block = Bottleneck</span><br><span class="line">        self.cb_block = self.block(2048, 2048 // 4, stride=1)</span><br><span class="line">        self.rb_block = self.block(2048, 2048 // 4, stride=1)</span><br><span class="line"></span><br><span class="line">    def initilize_all_parameters(self):</span><br><span class="line">        self.alpha = 0.2</span><br><span class="line">        <span class="keyword">if</span> self.epoch_number <span class="keyword">in</span> [90, 180]:</span><br><span class="line">            self.div_epoch = 100 * (self.epoch_number // 100 + 1)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.div_epoch = self.epoch_number</span><br><span class="line"></span><br><span class="line">    def forward(self, <span class="built_in">source</span>, meta, epoch):</span><br><span class="line"></span><br><span class="line">        image_a, image_b = <span class="built_in">source</span>[<span class="string">&quot;image&quot;</span>], meta[<span class="string">&quot;sample_image&quot;</span>]</span><br><span class="line"></span><br><span class="line">        feature_a, feature_b = (</span><br><span class="line">            (self.module(self.cb_block(self.backbone(image_a)))).view(image_a.shape[0], -1),</span><br><span class="line">            (self.module(self.rb_block(self.backbone(image_b)))).view(image_a.shape[0], -1),</span><br><span class="line">        )</span><br><span class="line">        l = 1 - (epoch / self.div_epoch) ** 2  <span class="comment"># parabolic decay</span></span><br><span class="line">        <span class="comment"># l = 0.5  # fix</span></span><br><span class="line">        <span class="comment"># l = math.cos((self.epoch-1) / self.div_epoch * math.pi /2)   # cosine decay</span></span><br><span class="line">        <span class="comment"># l = 1 - (1 - ((self.epoch - 1) / self.div_epoch) ** 2) * 1  # parabolic increment</span></span><br><span class="line">        <span class="comment"># l = 1 - (self.epoch-1) / self.div_epoch  # linear decay</span></span><br><span class="line">        <span class="comment"># l = np.random.beta(self.alpha, self.alpha) # beta distribution</span></span><br><span class="line">        <span class="comment"># l = 1 if self.epoch &lt;= 120 else 0  # seperated stage</span></span><br><span class="line">        mixed_feature = 2 * torch.cat((l * feature_a, (1 - l) * feature_b), dim=1)</span><br><span class="line">        output = self.classifier(mixed_feature)</span><br><span class="line">        output = self.softmax(output)</span><br><span class="line">        <span class="built_in">return</span> output, l</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>分类</tag>
        <tag>不均衡</tag>
      </tags>
  </entry>
  <entry>
    <title>Self-supervised Structure Modeling for Object Recognition</title>
    <url>/Self-supervised%20Structure%20Modeling%20for%20Object%20Recognition/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Most object recognition approaches predominantly focus on learning discriminative visual patterns while overlooking the holistic object structure. Though important, structure modeling usually requires significant manual annotations and therefore is labor-intensive. In this paper, we propose to “look into object” (explicitly yet intrinsically model the object structure) through incorporating self-supervisions into the traditional framework. We show the recognition backbone can be substantially enhanced for more robust representation learning, without any cost of extra annotation and inference speed. Specifically, we first propose an object-extent learning module for localizing the object according to the visual patterns shared among the instances in the same category. We then design a spatial context learning module for modeling the internal structures of the object, through predicting the relative positions within the extent. These two modules can be easily plugged into any backbone networks during training and detached at inference time. Extensive experiments show that our look-into-object approach (LIO) achieves large performance gain on a number of benchmarks, including generic object recognition (ImageNet) and fine-grained object recognition tasks (CUB, Cars, Aircraft). We also show that this learning paradigm is highly generalizable to other tasks such as object detection and segmentation (MS COCO). Project page: <a href="https://github.com/JDAI-CV/LIO">https://github.com/JDAI-CV/LIO</a>.<span id="more"></span></p>
<p>大多数的物体识别方法主要集中在学习有区别的视觉模式，而忽略了整体的物体结构。虽然很重要，但结构建模通常需要大量的手工注释，因此是劳动密集型的。在本文中，我们建议通过将自我监督整合到传统框架中来“研究对象”(明确而本质地建模对象结构)。我们证明了识别主干可以被大大增强，以获得更健壮的表示学习，而无需额外的注释和推理速度的代价。具体来说，我们首先提出一个对象范围学习模块，用于根据同一类别中的实例之间共享的可视化模式定位对象。然后，我们设计了一个空间情境学习模块，通过预测物体在范围内的相对位置来建模物体的内部结构。这两个模块可以很容易地在训练期间插入任何骨干网，并在推理时分离。大量的实验表明，我们的寻找对象方法(LIO)在一些基准测试中获得了很大的性能提升，包括通用对象识别(ImageNet)和细粒度对象识别任务(CUB, Cars, Aircraft)。我们还表明，这种学习模式是高度可推广到其他任务，如目标检测和分割(MS COCO)。项目页面:<a href="https://github.com/JDAI-CV/LIO。">https://github.com/JDAI-CV/LIO。</a></p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><h2 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h2><p>•分类模块(CM):提取基本图像表示并产生最终对象类别的主干分类网络。<br>•对象范围学习模块(Object-Extent Learning, OEL):一个用于定位给定图像中的主要对象的模块。<br>•空间上下文学习模块(Spatial Context Learning Module, SCL):一个自我监督的模块，通过CM中特征单元(后面介绍)之间的相互作用来加强区域之间的联系。<br><img src="https://img-blog.csdnimg.cn/20210617104008566.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="分类模块-Classification-Module-CM"><a href="#分类模块-Classification-Module-CM" class="headerlink" title="分类模块 Classification Module(CM)"></a>分类模块 Classification Module(CM)</h2><p>给定图像$I$，标签$l$，通过基本网络得到特征图$f(I)$，特征图大小为$N×N×C$，通过池化全连接层得到最后的特征向量$y(I)$，通过交叉熵损失来优化网络。</p>
<script type="math/tex; mode=display">\mathcal{L}_{c l s}=-\sum_{I \in \mathcal{I}} l \cdot \log \boldsymbol{y}(I)</script><p>OEL模块和SCL模块旨在帮助我们的主干分类网络学习有利于结构理解和对象定位的表示。这两个模块都是轻量级的，只引入了少量的可学习参数。此外，在推理时禁用OEL和SCL，只需要分类模块来提高计算效率。</p>
<h2 id="对象范围学习模块Object-Extent-Learning-OEL"><a href="#对象范围学习模块Object-Extent-Learning-OEL" class="headerlink" title="对象范围学习模块Object-Extent Learning (OEL)"></a>对象范围学习模块Object-Extent Learning (OEL)</h2><p>1.将特征图$f(I)$在像素维度上分成$N×N$个特征向量$\boldsymbol{f}(I)_{i, j} \in \mathbb{R}^{1 \times C}$$(1 \leq i, j \leq N)$，每个特征向量集中响应输入图像I中的某个区域。<br>2.采样和图片$I$具有相同标签的数据$\boldsymbol{I}^{\prime}=\left\{I_{1}^{\prime}, I_{2}^{\prime}, \cdots, I_{P}^{\prime}\right\}$，测量$f(I)_{i, j}$与同标签其他每幅图像$I^{\prime} \in \boldsymbol{I}^{\prime}$之间的区域级相关性。</p>
<script type="math/tex; mode=display">\varphi_{i, j}\left(I, I^{\prime}\right)=\frac{1}{C} \max _{1 \leq i^{\prime}, j^{\prime} \leq N}\left\langle\boldsymbol{f}(I)_{i, j}, \boldsymbol{f}\left(I^{\prime}\right)_{i^{\prime}, j^{\prime}}\right\rangle</script><p>$\langle\cdot, \cdot\rangle$为点乘<br>—————————————————————————————————————<br><strong>关于这个公式联系下图开会时我也思考了一下：同类图片得到特征图之后，分成N*N份特征向量单元，点乘可以评估两个特征向量的相似程度，因此对每个特征单元分别进行点乘后选取最大的即表明对应的区域相似度最高，由此产生了对应于整个物体的特征。</strong><br>—————————————————————————————————————<br>3.与分类$L_{cls}$联合训练，相关分数的$\varphi_{i, j}\left(I, I^{\prime}\right)$通常与$l$的语义相关度呈正相关关系。<br>4.这种语义相关掩码$\varphi$可以很好地捕获来自同一类别的图像的共性，其中$\varphi$中的值自然地区分了主要目标区域和背景。考虑到视点变化和变形的影响，使用多个正图像来定位一个对象的主要区域。</p>
<script type="math/tex; mode=display">M\left(I, \boldsymbol{I}^{\prime}\right)=\frac{1}{P} \sum_{p=1}^{P} \varphi\left(I, I_{p}^{\prime}\right)</script><p>$M\left(I, \boldsymbol{I}^{\prime}\right)$也可以被认为是同一类别图像之间共有的表征。</p>
<p><img src="https://img-blog.csdnimg.cn/20210616095026176.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">5.在得到特征图$f(I)$后，对$f(I)$中所有特征映射进行加权融合。对特征进行1 × 1卷积处理，得到单通道的输出$m^{\prime}(I)$(实际上就是将特征集合起来以此得到目标的整体结构)。与传统的注意力的目的是检测某些特定的部分或区域不同，我们的OEL模块被训练为收集物体内部的所有区域，忽略背景或其他无关的物体。<br>OEL模块$\mathcal{L}_{\text {oel }}$可以定义为对象范围的伪掩模$M\left(I, \boldsymbol{I}^{\prime}\right)$与$m^{\prime}(I)$之间的距离:</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text {oel }}=\sum_{I \in \mathcal{I}} \operatorname{MSE}\left(m^{\prime}(I), M\left(I, \boldsymbol{I}^{\prime}\right)\right)</script><h2 id="空间上下文学习模块Spatial-Context-Learning-SCL"><a href="#空间上下文学习模块Spatial-Context-Learning-SCL" class="headerlink" title="空间上下文学习模块Spatial Context Learning (SCL)"></a>空间上下文学习模块Spatial Context Learning (SCL)</h2><p>此模块同样作用于$f(I)$，目的是学习区域之间的结构关系。首先，对$f(I)$进行1 × 1卷积加ReLU处理，得到新映射$\boldsymbol{h}(I) \in \mathbb{R}^{N \times N \times C_{1}}$，描述了不同特征元的空间信息。$\boldsymbol{h}(I)$中的每个cell(每个i, j位置)集中代表了图像$I$中一个区域的语义信息。通过建立不同区域之间的<strong>空间连接</strong>，可以很容易地建模对象不同部分之间的结构关系。<br>1.选用极坐标系来建模这种结构关系。$N×N$平面作为极坐标平面，原点为特征中值最大的位置$R_{o}=R_{x, y}$，区域$R_{i, j}$的极坐标可以写成$\left(\Gamma_{i, j}, \theta_{i, j}\right)$，此作为自监督的标签。<script type="math/tex">\begin{aligned} \Gamma_{i, j} &=\sqrt{(x-i)^{2}+(y-j)^{2}} / \sqrt{2} N \\ \theta_{i, j} &=(\operatorname{atan2}(y-j, x-i)+\pi) / 2 \pi \end{aligned}</script><br><img src="https://img-blog.csdnimg.cn/20210616201937356.png" alt="atan2"><br><img src="https://img-blog.csdnimg.cn/20210617104419668.png" alt="在这里插入图片描述"></p>
<p>2.对每个cell $h(I)_{i, j}$和原点处$h(I)_{x, y}$在通道维度上进行级联，通过全连接层和relu激活函数，得到预测的极坐标$\left(\Gamma_{i, j}^{\prime}, \theta_{i, j}^{\prime}\right)$，从OEL模块学到的对象范围掩码$m^{\prime}(I)$(one channel)也适用于SCL模块。通过以下损失函数进行优化。</p>
<script type="math/tex; mode=display">\mathcal{L}_{d i s}=\sum_{I \in \mathcal{I}} \sqrt{\frac{\sum_{1 \leq i, j \leq N} m^{\prime}(I)_{i, j}\left(\Gamma_{i, j}^{\prime}-\Gamma_{i, j}\right)^{2}}{\sum m^{\prime}(I)}}</script><script type="math/tex; mode=display">\mathcal{L}_{\angle}=\sum_{I \in \mathcal{I}} \sqrt{\frac{\sum_{1 \leq i, j \leq N} m^{\prime}(I)_{i, j}\left(\theta_{\Delta_{i, j}}-\bar{\theta}_{\Delta}\right)^{2}}{\sum m^{\prime}(I)}}</script><p>$\theta_{\Delta_{i, j}}=\left\{\begin{array}{ll}\theta_{i, j}^{\prime}-\theta_{i, j}, &amp; \text { if } \theta_{i, j}^{\prime}-\theta_{i, j} \geq 0 \\ 1+\theta_{i, j}^{\prime}-\theta_{i, j}, &amp; \text { otherwise }\end{array}\right.$<br>$\bar{\theta}_{\Delta}=\frac{1}{\sum m^{\prime}(I)} \sum_{1 \leq i, j \leq N} m^{\prime}(I)_{i, j} \theta_{\Delta_{i, j}}$<br>3.SCL模块的损失为：</p>
<script type="math/tex; mode=display">\mathcal{L}_{s c l}=\mathcal{L}_{d i s}+\mathcal{L}_{L}</script><h2 id="联合训练"><a href="#联合训练" class="headerlink" title="联合训练"></a>联合训练</h2><p>分类、对象范围学习和空间上下文学习模块以端到端方式进行训练</p>
<script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{c l s}+\alpha \mathcal{L}_{o e l}+\beta \mathcal{L}_{s c l}</script><p>$\alpha=\beta=0.1$</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>细粒度</tag>
        <tag>分类</tag>
        <tag>自监督</tag>
      </tags>
  </entry>
  <entry>
    <title>The Devil is in the Channels Mutual-Channel Loss for Fine-Grained Image Classification</title>
    <url>/The%20Devil%20is%20in%20the%20Channels%20Mutual-Channel%20Loss%20for%20Fine-Grained%20Image%20Classification/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Key for solving fine-grained image categorization is finding discriminate and local regions that correspond to subtle visual traits. Great strides have been made, with complex networks designed specifically to learn part-level discriminate feature representations. In this paper, we show it is possible to cultivate subtle details without the need for overly complicated network designs or training mechanisms – a single loss is all it takes. The main trick lies with how we delve into individual feature channels early on, as opposed to the convention of starting from a consolidated feature map. The proposed loss function, termed as mutual-channel loss (MC-Loss), consists of two channel-specific components: a discriminality component and a diversity component. The discriminality component forces  all feature channels belonging to the same class to be discriminative, through a novel channel-wise attention mechanism. The diversity component additionally constraints channels so that they become mutually exclusive on spatial-wise. The end result is therefore a set of feature channels that each reflects different locally discriminative regions for a specific class. The MC-Loss can be trained end-to-end, without the need for any bounding-box/part annotations, and yields highly discriminative regions during inference. Experimental results show our MC-Loss when implemented on top of common base networks can achieve state-of-the-art performance on all four fine-grained categorization datasets (CUB-Birds, FGVC-Aircraft, Flowers-102, and Stanford-Cars). Ablative studies further demonstrate the superiority of MC-Loss when compared with other recently proposed general-purpose losses for visual classification, on two different base networks. Code available at <a href="https://github.com/dongliangchang/Mutual-Channel-Loss">https://github.com/dongliangchang/Mutual-Channel-Loss</a><span id="more"></span></p>
<p>解决细粒度图像分类的关键是找到与细微视觉特征相对应的区分区域和局部区域。复杂的网络专门设计用来学习零件级的区别特征表示已经取得了长足的进步。在这篇文章中，我们展示了在不需要过于复杂的网络设计或训练机制的情况下培养微妙的细节是可能的——只需一个损失就可以了。主要的诀窍在于我们如何在早期深入研究各个特征通道，而不是从统一的特征图开始的惯例。所提出的损失函数称为互通道损失(MC-Loss)，由两个特定通道的组件组成：判别性组件和差异性组件。判别性组件通过一种新颖的通道注意力机制，强制属于同一类别的所有特征频道具有鉴别性。差异性组件另外约束通道，使得它们在空间上变得相互排斥。因此，最终结果是一组特征通道，每个通道反映特定类别的不同局部区分区域。MC-Loss可以端到端训练，不需要任何额外的bounding box/part标注，并在推理过程中产生高度可分辨的区域。实验结果表明，当我们的MCLoss在公共基础网络上实现时，可以在所有四个细粒度分类数据集(Cub-Birds、FGVC-Aircraft、Flowers102和Stanford-Cars)上获得最先进的性能。消融研究进一步证明，在两种不同的基础网络上，MC-Loss与最近提出的其他用于视觉分类的通用损失相比具有优越性。代码可在<a href="https://github.com/dongliangchang/Mutual-Channel-Loss获得">https://github.com/dongliangchang/Mutual-Channel-Loss获得</a></p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p><img src="https://img-blog.csdnimg.cn/20210405152547481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt=""><br>———————————————————————————————————————<br>将图像输入到基本网络提取特征得到$\mathcal{F} \in R^{N \times W \times H}$，其中需要将通道$N$设置成$c \times \xi$，其中，$c$为类别数量，$\xi$为平坦到每个类上的通道数。<br>由此第i个类别的特征可以表示为：</p>
<script type="math/tex; mode=display">\mathbf{F}_{i}=\left\{\mathcal{F}_{i \times \xi+1}, \mathcal{F}_{i \times \xi+2}, \cdots, \mathcal{F}_{i \times \xi+\xi}\right\}</script><p>之前提取的特征就可以表示为：</p>
<script type="math/tex; mode=display">\mathbf{F}=\left\{\mathbf{F}_{0}, \mathbf{F}_{1}, \cdots, \mathbf{F}_{c-1}\right\}</script><p>随后，$F$进入网络的两个流，其中两个不同的损失针对两个不同的目标定制。在上图中，$L_{C E}$流将$F$视为输入到全连接层中，作交叉熵损失，在这里，交叉熵损失鼓励网络提取主要集中在全局判别区域的信息特征。$L_{M C}$流监督网络聚焦不同的局部区分性区域，对$L_{M C}$做一个加权得到总的损失：</p>
<script type="math/tex; mode=display">\operatorname{Loss}(\mathbf{F})=L_{C E}(\mathbf{F})+\mu \times L_{M C}(\mathbf{F})</script><p>再来具体看$L_{M C}$流：<br><img src="https://img-blog.csdnimg.cn/20210405154516803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="判别组件"><a href="#判别组件" class="headerlink" title="判别组件"></a>判别组件</h2><p>上图（a）左，根据前面划分的$\mathbf{F}=\left\{\mathbf{F}_{0}, \mathbf{F}_{1}, \cdots, \mathbf{F}_{c-1}\right\}$，判别组件要求特征通道与类对齐，每个对应于特定类的特征通道都应该具有足够的判别能力。</p>
<script type="math/tex; mode=display">L_{d i s}(\mathbf{F})=L_{C E}(\boldsymbol{y}, \underbrace{\frac{\left[e^{g\left(\mathbf{F}_{0}\right)}, e^{g\left(\mathbf{F}_{1}\right)}, \cdots, e^{\left.g\left(\mathbf{F}_{c-1}\right)\right]^{\mathrm{T}}}\right.}{\sum_{i=0}^{c-1} e^{g\left(\mathbf{F}_{i}\right)}}}_{\text {Softmax }})</script><p>其中，$g\left(\mathbf{F}_{i}\right)=\underbrace{\frac{1}{W H} \sum_{k=1}^{W H}}_{\text {GAP }} \underbrace{\max _{j=1,2, \cdots, \xi}}_{\text {CCMP }} \underbrace{\left[M_{i} \cdot \mathbf{F}_{i, j, k}\right]}_{\text {CWA }}$；$M_{i}=\operatorname{diag}\left(\right.$ Mask $\left._{i}\right)$；Mask $_{i} \in R^{\xi}$ is a 0-1 mask with randomly $\left\lfloor\frac{\xi}{2}\right\rfloor$zero(s)；GAP，CCMP，CWA，分别为Global Average Pooling，Cross-Channel Max Pooling，Channel-Wise Attention。</p>
<h2 id="差异性组件"><a href="#差异性组件" class="headerlink" title="差异性组件"></a>差异性组件</h2><p>差异性组件是用于所有特征通道的近似距离测量，以计算所有通道的总相似度。与欧几里德距离和二次复杂度的Kullback-Leibler散度等常用度量相比，在计算复杂度不变的情况下，它的计算成本更低。沿着上图(a)的右侧块所示的差异性组件让组$L_{i}$中的特征通道通过训练变得彼此不同。换言之，一个类别的不同特征通道应该聚焦于图像的不同区域，而不是所有通道都聚焦于最具区分性的区域。因此，它通过使每个组的特征通道多样化来减少冗余信息，并有助于发现针对图像中每一类的不同区分区域。该操作可以被解释为跨通道去相关，以便从图像的不同显著区域捕捉细节。在Softmax之后，通过引入CCMP直接在卷积滤波器上施加监督，然后进行空间维度求和来衡量相交程度。$L_{div}$表示为：</p>
<script type="math/tex; mode=display">L_{d i v}(\mathbf{F})=\frac{1}{c} \sum_{i=0}^{c-1} h\left(\mathbf{F}_{i}\right)</script><p>其中，$h\left(\mathbf{F}_{i}\right)=\sum_{k=1}^{W H} \underbrace{\max _{j=1,2, \cdots, \xi}}_{C C M P} \underbrace{\left[\frac{e^{\mathbf{F}_{i, j, k}}}{\sum_{k^{\prime}=1}^{W H} e^{\mathbf{F}_{i, j, k^{\prime}}}}\right]}_{\text {Softmax }}$</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">def Mask(nb_batch, channels):</span><br><span class="line"></span><br><span class="line">    foo = [1] * 2 + [0] *  1</span><br><span class="line">    bar = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(200):</span><br><span class="line">        random.shuffle(foo)</span><br><span class="line">        bar += foo</span><br><span class="line">    bar = [bar <span class="keyword">for</span> i <span class="keyword">in</span> range(nb_batch)]</span><br><span class="line">    bar = np.array(bar).astype(<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    bar = bar.reshape(nb_batch,200*channels,1,1)</span><br><span class="line">    bar = torch.from_numpy(bar)</span><br><span class="line">    bar = bar.cuda()</span><br><span class="line">    bar = Variable(bar)</span><br><span class="line">    <span class="built_in">return</span> bar</span><br><span class="line"></span><br><span class="line">def supervisor(x,targets,height,cnum):</span><br><span class="line">    mask = Mask(x.size(0), cnum)</span><br><span class="line">    branch = x</span><br><span class="line">    branch = branch.reshape(branch.size(0),branch.size(1), branch.size(2) * branch.size(3))</span><br><span class="line">    branch = F.softmax(branch,2)</span><br><span class="line">    branch = branch.reshape(branch.size(0),branch.size(1), x.size(2), x.size(2))</span><br><span class="line">    branch = my_MaxPool2d(kernel_size=(1,cnum), stride=(1,cnum))(branch)  </span><br><span class="line">    branch = branch.reshape(branch.size(0),branch.size(1), branch.size(2) * branch.size(3))</span><br><span class="line">    loss_2 = 1.0 - 1.0*torch.mean(torch.sum(branch,2))/cnum <span class="comment"># set margin = 3.0</span></span><br><span class="line"></span><br><span class="line">    branch_1 = x * mask </span><br><span class="line"></span><br><span class="line">    branch_1 = my_MaxPool2d(kernel_size=(1,cnum), stride=(1,cnum))(branch_1)  </span><br><span class="line">    branch_1 = nn.AvgPool2d(kernel_size=(height,height))(branch_1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    branch_1 = branch_1.view(branch_1.size(0), -1)</span><br><span class="line"></span><br><span class="line">    loss_1 = criterion(branch_1, targets)</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">return</span> [loss_1, loss_2] </span><br><span class="line"></span><br><span class="line">class model_bn(nn.Module):</span><br><span class="line">    def __init__(self, feature_size=512,classes_num=200):</span><br><span class="line"></span><br><span class="line">        super(model_bn, self).__init__() </span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(*list(net.children())[:-2]) </span><br><span class="line"></span><br><span class="line">        self.max = nn.MaxPool2d(kernel_size=14, stride=14)</span><br><span class="line"></span><br><span class="line">        self.num_ftrs = 600*1*1</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.BatchNorm1d(self.num_ftrs),</span><br><span class="line">            <span class="comment">#nn.Dropout(0.5),</span></span><br><span class="line">            nn.Linear(self.num_ftrs, feature_size),</span><br><span class="line">            nn.BatchNorm1d(feature_size),</span><br><span class="line">            nn.ELU(inplace=True),</span><br><span class="line">            <span class="comment">#nn.Dropout(0.5),</span></span><br><span class="line">            nn.Linear(feature_size, classes_num),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x, targets):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = self.features(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            MC_loss = supervisor(x,targets,height=14,cnum=3)</span><br><span class="line"></span><br><span class="line">        x = self.max(x)</span><br><span class="line">        x = x.view(x.size(0), -1)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        loss = criterion(x, targets)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="built_in">return</span> x, loss, MC_loss</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">return</span> x, loss</span><br></pre></td></tr></table></figure>
<p>train</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">out, ce_loss, MC_loss = net(inputs, targets)</span><br><span class="line"></span><br><span class="line">loss = ce_loss + args[<span class="string">&quot;alpha_1&quot;</span>] * MC_loss[0] + args[<span class="string">&quot;beta_1&quot;</span>] * MC_loss[1] </span><br></pre></td></tr></table></figure>
<p>valid</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">out, ce_loss = net(inputs,targets)</span><br><span class="line">            </span><br><span class="line">test_loss += ce_loss.item()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>细粒度</tag>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Semantically Enhanced Feature for Fine-Grained Image Classification</title>
    <url>/Learning%20Semantically%20Enhanced%20Feature%20for%20Fine-Grained%20Image%20Classification/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>We aim to provide a computationally cheap yet effective approach for fine-grained image classification (FGIC) in this letter. Unlike previous methods that rely on complex part  localization modules, our approach learns fine-grained features by enhancing the semantics of sub-features of a global feature. Specifically, we first achieve the sub-feature semantic by arranging feature channels of a CNN into different groups through channel permutation. Meanwhile, to enhance the discriminability of sub-features, the groups are guided to be activated on object parts with strong discriminability by a weighted combination regularization. Our approach is parameter parsimonious and can be easily integrated into the backbone model as a plug-and-play module for end-to-end training with only image-level supervision. Experiments verified the effectiveness of our approach and validated its comparable performance to the state-of-the-artmethods. Code is available at https:// github.com/ cswluo/ SEF<span id="more"></span></p>
<p>本文旨在为细粒度图像分类(FGIC)提供一种计算量小但效果好的方法。与以往依赖复杂part定位模块的方法不同，我们的方法通过<strong>增强全局特征子特征</strong>的语义来学习细粒度特征。具体地说，我们首先通过通道排列将CNN的特征频道分成不同的组来实现子特征语义。同时，为了提高子特征的可区分性，通过加权组合正则化，引导分组在可区分性较强的object parts被激活。我们的方法参数很少，可以很容易地集成到主干模型中，作为即插即用模块，用于端到端培训，只需图像级别的监督。实验验证了该方法的有效性，并验证了其性能可与最先进的方法相媲美。有关代码，请访问<a href="https://github.com/cswluo/sef">https://github.com/cswluo/sef</a></p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p><img src="https://img-blog.csdnimg.cn/20210404103011877.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="图1.整体框架"><br>图1.整体框架.<br>语义分组模块将CNN的最后一层卷积特征通道(用混合色块表示)分成不同的组(用不同的颜色表示)。全局特征及其子特征(分组特征)通过平均池化从排列的特征通道中获得。灰色块中的淡黄色块表示对应的子特征的预测类分布，这些子特征通过knowledge distillation（知识蒸馏）得到的全局特征的输出进行正则化。所有灰块只在训练阶段有效，而在测试阶段去除。为了清楚起见，省略了CNN的细节。</p>
<h2 id="语义分组模块"><a href="#语义分组模块" class="headerlink" title="语义分组模块"></a>语义分组模块</h2><p>在CNN的高层中需要使用多个filters来表示语义概念。因此，作者开发了一种正则化方法，将具有不同属性的filters分成不同的组来捕获语义概念。</p>
<script type="math/tex; mode=display">\mathbf{X}^{L^{\prime}}=\mathbf{A} \mathbf{X}^{L}=\mathbf{A} \mathbf{B} \mathbf{X}^{L-1}=\mathbf{W} \mathbf{X}^{L-1}</script><p>$\mathbf{X}^{L^{\prime}}$ denotes the feature map with its feature channels arranged by a permutation operation<br>$\mathbf{A} \in \mathbb{R}^{C \times C}$ is a permutation matrix<br>$\mathbf{B} \in \mathbb{R}^{C \times \Omega}$ denotes the  reshaped filters of layer $\mathbf{L}$,<br>$\mathbf{X}^{L-1} \in \mathbb{R}^{\Omega \times \Psi}$ denotes the reshaped feature of layer $\mathbf{L-1}$<br>$\mathbf{W}$ is a permutation of $\mathbf{B}$.<br>要获得具有语义的组，$\mathbf{A}$应该学会发现B的过滤器(行)之间的相似性。然而，要直接学习排列矩阵并不是一件容易的事。因此，作者直接通过约束$\mathbf{X}^{L^{\prime}}$的特征通道之间的关系来学习$\mathbf{W}$，从而绕过了学习$\mathbf{A}$的困难。为了达到效果，作者最大化了同一组中的特征通道之间的相关性，同时解除了不同组中的特征通道之间的相关性，依靠损失函数 LocalMaxGlobalMin loss：</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text {group }}=\frac{1}{2}\left(\|\mathbf{D}\|_{F}^{2}-2\|\operatorname{diag}(\mathbf{D})\|_{2}^{2}\right)</script><p>$\tilde{\mathbf{X}}_{i}^{L^{\prime}} \leftarrow \mathbf{X}_{i}^{L^{\prime}} /\left|\mathbf{X}_{i}^{L^{\prime}}\right|_{2}$作为一个normalized channel<br>$d_{i j}=\tilde{\mathbf{X}}_{i}^{L^{\prime T}} \tilde{\mathbf{X}}_{j}^{L^{\prime}}$<br>$\mathrm{D} \in \mathbb{R}^{G \times G}$<br>$\mathbf{D}$中的元素$\mathbf{D}_{m n}=\frac{1}{C_{m} C n} \sum_{i \in m, j \in n} d_{i j}$</p>
<p>LocalMaxGlobalMin loss 实现代码</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">class LocalMaxGlobalMin(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, rho, nchannels, nparts=1, device=<span class="string">&#x27;cpu&#x27;</span>):</span><br><span class="line">        super(LocalMaxGlobalMin, self).__init__()</span><br><span class="line">        self.nparts = nparts</span><br><span class="line">        self.device = device</span><br><span class="line">        self.nchannels = nchannels</span><br><span class="line">        self.rho = rho</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        nlocal_channels_norm = nchannels // self.nparts</span><br><span class="line">        reminder = nchannels % self.nparts</span><br><span class="line">        nlocal_channels_last = nlocal_channels_norm</span><br><span class="line">        <span class="keyword">if</span> reminder != 0:</span><br><span class="line">            nlocal_channels_last = nlocal_channels_norm + reminder</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># seps records the indices partitioning feature channels into separate parts</span></span><br><span class="line">        seps = []</span><br><span class="line">        sep_node = 0</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">            <span class="keyword">if</span> i != self.nparts-1:</span><br><span class="line">                sep_node += nlocal_channels_norm                </span><br><span class="line">                <span class="comment">#seps.append(sep_node)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sep_node += nlocal_channels_last                </span><br><span class="line">            seps.append(sep_node)</span><br><span class="line">        self.seps = seps</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):  </span><br><span class="line">        x = x.pow(2)</span><br><span class="line">        intra_x = []</span><br><span class="line">        inter_x = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">            <span class="keyword">if</span> i == 0:        </span><br><span class="line">                intra_x.append((1 - x[:, :self.seps[i], :self.seps[i]]).mean()) </span><br><span class="line">            <span class="keyword">else</span>:              </span><br><span class="line">                intra_x.append((1 - x[:, self.seps[i-1]:self.seps[i], self.seps[i-1]:self.seps[i]]).mean())</span><br><span class="line">                inter_x.append(x[:, self.seps[i-1]:self.seps[i], :self.seps[i-1]].mean())</span><br><span class="line">        </span><br><span class="line">        loss = self.rho * 0.5 * (sum(intra_x) / self.nparts + sum(inter_x) / (self.nparts*(self.nparts-1)/2)) </span><br><span class="line">                 </span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="特征增强模块"><a href="#特征增强模块" class="headerlink" title="特征增强模块"></a>特征增强模块</h2><p>语义分组可以驱动不同组的特征在不同的语义(对象)部分上被激活。然而，这些部分的可识别性可能得不到保证。因此，需要引导这些语义组在具有很强区分度的对象部分上被激活。实现此效果的一种简单方法是<strong>匹配对象及其部分之间的预测分布(即知识蒸馏，我理解成全局和局部之间的分布学习)</strong>，匹配分布可以利用KL散度。</p>
<script type="math/tex; mode=display">\mathcal{L}_{\mathrm{KL}\left(\mathbf{P}_{w} \| \mathbf{P}_{a}\right)}=-\mathrm{H}\left(\mathbf{P}_{w}\right)+\mathrm{H}\left(\mathbf{P}_{w}, \mathbf{P}_{a}\right)</script><p>$\mathbf{P}_{w}$ and $\mathbf{P}_{a}$ are the prediction distributions of an object and its part<br>(即全局特征和局部特征)<br>$\mathrm{H}\left(\mathbf{P}_{w}\right)=-\sum \mathbf{P}_{w} \log \mathbf{P}_{w}$<br>$\mathrm{H}\left(\mathbf{P}_{w}, \mathbf{P}_{a}\right)=-\sum \mathbf{P}_{w} \log \mathbf{P}_{a}$</p>
<p>因此得到这一模块得损失函数：</p>
<script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{c r}-\lambda \mathrm{H}\left(\mathbf{P}_{w}\right)+\lambda \mathrm{H}\left(\mathbf{P}_{w}, \mathbf{P}_{a}\right)</script><p>$\mathcal{L}_{c r}$是全局特征预测的交叉熵损失</p>
<p>将两个模块的损失函数加权相加得到最终的损失：</p>
<script type="math/tex; mode=display">\mathcal{L}=\mathbb{E}_{\mathbf{x}}\left(\mathcal{L}_{c r}-\lambda \mathrm{H}\left(\mathbf{P}_{w}\right)+\frac{\gamma}{G} \sum \mathrm{H}\left(\mathbf{P}_{w}, \mathbf{P}_{a}^{g}\right)+\phi \mathcal{L}_{\text {group }}\right)</script><h1 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h1><p>自定义nparts的大小，nparts表示分组的个数，以resnet50主干为例，将layer4输出的特征根据channel均分为nparts份。假设nparts=4，每份channel大小为512。将得到的nparts个特征图分别输入到不同的fc中，得到局部部分的预测xlocal，size为torch.Size([nparts, batchsize, num_classes]。生成一个排列矩阵xcos，输出后依赖此矩阵进行LocalMaxGlobalMin loss计算。<br>此外，对copy一份layer4输出的特征正常操作，得到全局的预测xglobal，size为torch.Size([batchsize, num_classes])<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加在Resnet类__init__方法里面</span></span><br><span class="line">        <span class="keyword">if</span> self.attention:            </span><br><span class="line">            nfeatures = 512 * block.expansion            </span><br><span class="line">            nlocal_channels_norm = nfeatures // self.nparts</span><br><span class="line">            reminder = nfeatures % self.nparts</span><br><span class="line">            nlocal_channels_last = nlocal_channels_norm</span><br><span class="line">            <span class="keyword">if</span> reminder != 0:</span><br><span class="line">                nlocal_channels_last = nlocal_channels_norm + reminder</span><br><span class="line">            fc_list = []</span><br><span class="line">            separations = []</span><br><span class="line">            sep_node = 0</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">                <span class="keyword">if</span> i != self.nparts-1:</span><br><span class="line">                    sep_node += nlocal_channels_norm</span><br><span class="line">                    fc_list.append(nn.Linear(nlocal_channels_norm, num_classes))</span><br><span class="line">                    <span class="comment">#separations.append(sep_node)</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    sep_node += nlocal_channels_last</span><br><span class="line">                    fc_list.append(nn.Linear(nlocal_channels_last, num_classes))</span><br><span class="line">                separations.append(sep_node)</span><br><span class="line">            self.fclocal = nn.Sequential(*fc_list)</span><br><span class="line">            self.separations = separations </span><br><span class="line">            self.fc = nn.Linear(512*block.expansion, num_classes) </span><br><span class="line">  —————————————————————————————————————————————————————————————————————————————————</span><br><span class="line">    <span class="comment"># Resnet类的forward</span></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)  <span class="comment"># [4,64,224,224]</span></span><br><span class="line">        x = self.bn1(x)  <span class="comment"># [4,64,224,224]</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)   <span class="comment"># [4,64,112,112]</span></span><br><span class="line">  </span><br><span class="line">        x = self.layer1(x)   <span class="comment"># [4,256,112,112]</span></span><br><span class="line">        x = self.layer2(x)   <span class="comment"># [4,512,56,56]</span></span><br><span class="line">        x = self.layer3(x)   <span class="comment"># [4,1024,28,28]</span></span><br><span class="line">        x = self.layer4(x)   <span class="comment"># [4,2048,14,14]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.attention:</span><br><span class="line"></span><br><span class="line">            nsamples, nchannels, height, width = x.shape</span><br><span class="line">        </span><br><span class="line">            xview = x.view(nsamples, nchannels, -1)  <span class="comment"># torch.Size([4, 2048, 196])</span></span><br><span class="line">            xnorm = xview.div(xview.norm(dim=-1, keepdim=True)+eps)  <span class="comment"># torch.Size([4, 2048, 196])</span></span><br><span class="line">            xcosin = torch.bmm(xnorm, xnorm.transpose(-1, -2))  <span class="comment"># torch.Size([4, 2048, 2048])</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            attention_scores = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.nparts):</span><br><span class="line">                <span class="keyword">if</span> i == 0:</span><br><span class="line">                    xx = x[:, :self.separations[i]]  <span class="comment"># torch.Size([4, 512, 14, 14])</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    xx = x[:, self.separations[i-1]:self.separations[i]]</span><br><span class="line">                xx_pool = self.avgpool(xx).flatten(1)  <span class="comment"># torch.Size([4, 512])</span></span><br><span class="line">                attention_scores.append(self.fclocal[i](xx_pool))</span><br><span class="line">            xlocal = torch.stack(attention_scores, dim=0)  <span class="comment"># torch.Size([4, 4, num_classes])</span></span><br><span class="line"></span><br><span class="line">            xmaps = x.clone().detach()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># for global</span></span><br><span class="line">            xpool = self.avgpool(x)</span><br><span class="line">            xpool = torch.flatten(xpool, 1)</span><br><span class="line">            xglobal = self.fc(xpool)  <span class="comment"># torch.Size([4, num_classes])</span></span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">            <span class="built_in">return</span> xglobal, xlocal, xcosin, xmaps</span><br></pre></td></tr></table></figure></p>
<p>train,val<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xglobal, xlocal, xcosin, _ = model(inputs)</span><br><span class="line">	probs = softmax(xglobal)                    </span><br><span class="line">cls_loss = criterion[0](xglobal, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################################## prediction</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># prediction of every  branch</span></span><br><span class="line">probl, predl, logprobl = [], [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nparts):</span><br><span class="line">	probl.append(softmax(torch.squeeze(xlocal[i])))</span><br><span class="line">	predl.append(torch.max(probl[i], 1)[-1])</span><br><span class="line">	logprobl.append(logsoftmax(torch.squeeze(xlocal[i])))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############################################################### regularization</span></span><br><span class="line"></span><br><span class="line">logprobs = logsoftmax(xglobal)</span><br><span class="line">entropy_loss = penalty[<span class="string">&#x27;entropy_weights&#x27;</span>] * torch.mul(probs, logprobs).sum().div(inputs.size(0))</span><br><span class="line">soft_loss_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nparts):</span><br><span class="line">	soft_loss_list.append(torch.mul(torch.neg(probs), logprobl[i]).sum().div(inputs.size(0)))</span><br><span class="line">soft_loss = penalty[<span class="string">&#x27;soft_weights&#x27;</span>] * sum(soft_loss_list).div(nparts)</span><br><span class="line"><span class="comment"># regularization loss</span></span><br><span class="line">lmgm_reg_loss = criterion[1](xcosin)</span><br><span class="line">reg_loss = lmgm_reg_loss + entropy_loss + soft_loss</span><br></pre></td></tr></table></figure></p>
<h1 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h1><p><img src="https://img-blog.csdnimg.cn/20210405090812119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h1><p><img src="https://img-blog.csdnimg.cn/20210405090733844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20210405090626358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210405090718371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>细粒度</tag>
        <tag>分类</tag>
        <tag>注意力</tag>
      </tags>
  </entry>
  <entry>
    <title>Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification</title>
    <url>/Weakly%20Supervised%20Data%20Augmentation%20Network%20for%20Fine-Grained%20Visual%20Classification/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiencyandmightintroducemanyuncontrolledbackground noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object’s discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts’ features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-ofthe-art methods, which demonstrates its effectiveness.<span id="more"></span></p>
<p>为了防止过度拟合并提高深度模型的性能，通常采用数据扩充的方法来增加训练数据量。然而，在实际应用中，随机数据增强（如随机图像裁剪）的效率较低，而且会引入许多不可控的背景噪声。在本文中，我们提出了弱监督数据扩充网络（WS-DAN）来探索数据扩充的潜力。具体地说，对于每幅训练图像，我们首先通过弱监督学习学习生成注意图来表示目标的判别部分。接下来，我们对这些注意力地图引导的图像进行增强，包括注意力裁剪和注意力丢弃。本文提出的WS-DAN算法将分类精度提高了两倍。在第一阶段，由于提取了更多有鉴别能力的零件的特征，因此可以更好地看到图像。在第二阶段，注意区域提供了目标的精确定位，保证了模型更近距离地观察目标，进一步提高了性能。在常见的细粒度视觉分类数据集上的综合实验表明，本文的WS-DAN方法优于现有的方法，证明了其有效性。</p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><h2 id="弱监督注意学习"><a href="#弱监督注意学习" class="headerlink" title="弱监督注意学习"></a>弱监督注意学习</h2><h3 id="空间表示法"><a href="#空间表示法" class="headerlink" title="空间表示法"></a>空间表示法</h3><p>这部分先介绍一些对象的符号表示<br>特征图 $F \in R^{H \times W \times C}$<br>注意力图 $A \in R^{H \times W \times M}$ (M&lt;=C)，从特征图中获取：</p>
<script type="math/tex; mode=display">A=f(F)=\bigcup_{k=1}^{M} A_{k}</script><p>$f$是一个卷积函数（这地方我看的很懵，官方给的代码里面attention map直接选择了前32通道的特征图，$f$即卷积操作没有用到？）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取注意力图的代码</span></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        identity = x  <span class="comment">## feature map</span></span><br><span class="line">        <span class="keyword">if</span> self.downsample is not None:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        feature_map = out  <span class="comment"># [4,512,7,7]</span></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)  <span class="comment"># [4,512,7,7]</span></span><br><span class="line">        <span class="keyword">if</span> self.use_bap:</span><br><span class="line">            attention = out[:,:32,:,:]  <span class="comment"># [4,32,7,7]</span></span><br><span class="line">            raw_features,pooling_features = self.bap(feature_map,attention)  <span class="comment"># [4,16384]  # [4,16384]</span></span><br><span class="line">            <span class="built_in">return</span> attention,raw_features,pooling_features</span><br></pre></td></tr></table></figure>
<h3 id="双线性池化"><a href="#双线性池化" class="headerlink" title="双线性池化"></a>双线性池化</h3><p>作者提出双线性注意池化(BAP)来提取这些零件部位(parts)的特征。首先，将特征图和每个注意力作元素乘法：</p>
<script type="math/tex; mode=display">F_{k}=A_{k} \odot F \quad(k=1,2, \ldots, M)</script><p>然后通过另外的特征提取函数$g(·)$，如全局平均池化（GAP）、全局最大池化（GMP）或卷积，进一步提取有区别的局部特征，得到k个目标特征$f_{k}∈R^{1×N}$，</p>
<script type="math/tex; mode=display">P=\Gamma(A, F)=\left(\begin{array}{c}g\left(a_{1} \odot F\right) \\ g\left(a_{2} \odot F\right) \\ \cdots \\ g\left(a_{M} \odot F\right)\end{array}\right)=\left(\begin{array}{c}f_{1} \\ f_{2} \\ \ldots \\ f_{M}\end{array}\right)</script><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 双线性给池化代码</span></span><br><span class="line">class Bilinear_Pooling(nn.Module):</span><br><span class="line">    def __init__(self,  **kwargs):</span><br><span class="line">        super(Bilinear_Pooling, self).__init__()</span><br><span class="line"></span><br><span class="line">    def forward(self, feature_map1, feature_map2):</span><br><span class="line">        N, D1, H, W = feature_map1.size()   <span class="comment"># [4,512,7,7]</span></span><br><span class="line">        feature_map1 = torch.reshape(feature_map1, (N, D1, H * W))   <span class="comment"># [4,512,49]</span></span><br><span class="line">        N, D2, H, W = feature_map2.size()   <span class="comment"># [4,32,7,7]</span></span><br><span class="line">        feature_map2 = torch.reshape(feature_map2, (N, D2, H * W))   <span class="comment"># [4,32,49]</span></span><br><span class="line">        X = torch.bmm(feature_map1, torch.transpose(feature_map2, 1, 2)) / (H * W)  <span class="comment">#点乘后平均</span></span><br><span class="line">        X = torch.reshape(X, (N, D1 * D2))   <span class="comment"># [4,32*512]</span></span><br><span class="line">        X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)  <span class="comment"># 不能为0</span></span><br><span class="line">        bilinear_features = 100 * torch.nn.functional.normalize(X)  <span class="comment">#计算范数，默认一维上作二范数</span></span><br><span class="line">        <span class="built_in">return</span> bilinear_features</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20210426111106252.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210426111128207.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMTE4OTY4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="Center-loss"><a href="#Center-loss" class="headerlink" title="Center loss"></a>Center loss</h3><script type="math/tex; mode=display">\mathcal{L}_{C}=\frac{1}{2} \sum_{i=1}^{m}\left\|\boldsymbol{x}_{i}-\boldsymbol{c}_{y_{i}}\right\|_{2}^{2}</script><p>${c}_{y_{i}}$表示第$y_{i}$个类别的特征中心，$x_{i}$表示全连接层之前的特征。$m$表示mini-batch的大小。因此这个公式就是希望一个batch中的每个样本的特征离特征中心的距离的平方和要越小越好，也就是类内距离要越小越好。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>细粒度</tag>
        <tag>分类</tag>
        <tag>弱监督</tag>
      </tags>
  </entry>
</search>
